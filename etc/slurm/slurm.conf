#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=compute-cluster
ControlMachine=goedel
#ControlAddr=
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
#SlurmdUser=root
# SlurmctldPort=6817
# SlurmdPort=6818
SlurmctldPort=46817
SlurmdPort=46818
SrunPortRange=60001-63000
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/cgroup
PluginDir=/usr/lib/slurm
#FirstJobId=
ReturnToService=1
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=/etc/slurm/prolog.d/*
#PrologFlags=Alloc
#Epilog=/etc/slurm/epilog.d/*
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
TaskPlugin=task/cgroup
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
# SelectType=select/cons_res
SelectType=select/cons_tres

# "Core"=physical CPU, "CPU"=thread
SelectTypeParameters=CR_Core_Memory,CR_ONE_TASK_PER_CORE

PriorityType=priority/multifactor
# 1 day half-life
PriorityDecayHalfLife=1-0
#PriorityUsageResetPeriod=14-0
PriorityWeightAssoc=0
PriorityWeightFairshare=100000
PriorityWeightAge=1000
#PriorityMaxAge=1-0
PriorityWeightPartition=1000
PriorityWeightJobSize=1000
PriorityFavorSmall=YES
PriorityWeightTRES=GRES/gpu=-1000
PriorityWeightQOS=0

# LOGGING
# set to "debug5" to get the most logging
SlurmctldDebug=error
SlurmctldLogFile=/var/log/slurmctld.log
# set to "debug5" to get the most logging
SlurmdDebug=error
SlurmdLogFile=/var/log/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/cgroup
#JobAcctGatherFrequency=30
#
AccountingStorageTRES=gres/gpu
DebugFlags=CPU_Bind,gres
AccountingStorageType=accounting_storage/slurmdbd
# AccountingStorageType=accounting_storage/none
AccountingStorageHost=goedel
#AccountingStorageLoc=
AccountingStoragePass=/var/run/munge/munge.socket.2
AccountingStorageUser=slurm
AccountingStoragePort=46819
DefaultStoragePort=46819
JobCompPort=46667
#
# COMPUTE NODES
GresTypes=gpu,mps
DefMemPerGPU=30000
DefCpuPerGPU=3

# NodeName=linux1 Gres=gpu:8 CPUs=80 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 RealMemory=515896 State=UNKNOWN
NodeName=goedel Gres=gpu:rtx_3090:7,mps:168 CPUs=24 Boards=1 SocketsPerBoard=1 CoresPerSocket=24 ThreadsPerCore=1 RealMemory=257626
NodeName=turing Gres=gpu:rtx_2080:2,mps:22 CPUs=128 Boards=1 SocketsPerBoard=1 CoresPerSocket=64 ThreadsPerCore=2 RealMemory=257777
NodeName=hydra Gres=gpu:titan:2,mps:24 CPUs=40 Boards=1 SocketsPerBoard=2 CoresPerSocket=10 ThreadsPerCore=2 RealMemory=64321
NodeName=fear Gres=gpu:quadro:1,mps:16 CPUs=20 Boards=1 SocketsPerBoard=1 CoresPerSocket=10 ThreadsPerCore=2 RealMemory=64124

# QUEUES
# default values
PartitionName=DEFAULT MaxTime=INFINITE State=UP
# queues
PartitionName=all Nodes=ALL PriorityJobFactor=200
PartitionName=goedel Nodes=goedel DefCpuPerGPU=3 Default=Yes PriorityJobFactor=100
PartitionName=turing Nodes=turing DefCpuPerGPU=8 PriorityJobFactor=100
PartitionName=hydra Nodes=hydra DefCpuPerGPU=10 PriorityJobFactor=100
PartitionName=fear Nodes=fear DefCpuPerGPU=10 DefMemPerGPU=60000 PriorityJobFactor=100
